{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebc7baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581b8a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8bfb03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling PyTorch because PyTorch >= 2.1 is required but found 2.0.0\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "import torch\n",
    "\n",
    "from ChEmbed.data import chembldb, smiles_dataset, chembed_tokenize\n",
    "from ChEmbed.training import trainer\n",
    "from ChEmbed.modules import simple_rnn\n",
    "import attr\n",
    "\n",
    "from ChEmbed import plots, utilities\n",
    "\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c6ef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "chembl_raw = chembldb.ChemblDB()\n",
    "chembl_smiles = chembl_raw._load_or_download()[\"canonical_smiles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "197807b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = chembed_tokenize.load_chembed_tokenizer(filepath = \"../data/tokenizers/tokenizer-chembldb-16-06-2025.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23c0aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chembl_dataset = smiles_dataset.SMILESDataset(\n",
    "#     smiles_list = chembl_smiles,\n",
    "#     tokenizer = tokenizer\n",
    "# )\n",
    "\n",
    "# chembl_mini = smiles_dataset.SMILESDataset(\n",
    "#     smiles_list = chembl_smiles[:5000],\n",
    "#     tokenizer = tokenizer\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2053b2a0-27eb-4a75-873a-b92072beea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "chembl_mini = smiles_dataset.SMILESDatasetContinuous(\n",
    "    smiles_list = chembl_smiles[:25000],\n",
    "    tokenizer = tokenizer,\n",
    "    length = 1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76de057-ef1b-429a-9e0e-a0dd792180db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a619e5c-044d-4b65-a256-c73f2772e5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chembl_mini[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c378a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/miniconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "model = simple_rnn.simpleRNN(\n",
    "    # Mandatory\n",
    "    num_hiddens = 128,\n",
    "    vocab_size = len(tokenizer),\n",
    "    # tuning\n",
    "    learning_rate = 0.2,\n",
    "    weight_decay = 0.08\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be0fdc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input, num_preds, model, dataset, device=None):\n",
    "    state, outputs = None, [input[0]]\n",
    "\n",
    "    # predict input + num_preds tokens\n",
    "    for i in range(len(input) + num_preds - 1):\n",
    "        \n",
    "        X = torch.tensor(dataset.encode_smiles_to_one_hot(outputs[i]), device=device)\n",
    "        \n",
    "        rnn_outputs = model(X, state)\n",
    "        \n",
    "        if i < len(input) - 1:\n",
    "            outputs.append(input[i + 1])\n",
    "        else:\n",
    "            tokens = int(rnn_outputs[:, -1].argmax())\n",
    "            Y = dataset.tokenizer.decode(tokens)\n",
    "            outputs.append(Y)\n",
    "    return ''.join(outputs)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6bd5bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_57637/4192907761.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(dataset.encode_smiles_to_one_hot(outputs[i]), device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCCC1OC1OC1OC1OC1OC1OC1OC1OC1OC1OC1OC1OC1OC1OC1OC1OC1OC1OC1OC1OC1OC1OC1OC1OC1OC1OC1OC1OC1OC1O\n"
     ]
    }
   ],
   "source": [
    "print(predict(\"CCC\", 30, model, chembl_mini, device=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecc88c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 5/5... (Epoch 1/16)"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_trainer \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, init_random\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, clip_grads_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchembl_mini\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/documents/code/ChEmbed/notebooks/../../ChEmbed/training/trainer.py:78\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, data)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epochs,\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_train_batches\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train_batches,\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_val_batches\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_val_batches,\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m     73\u001b[0m }\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epochs):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# one epoch fits over the entire training dataset\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# then evaluates on validation dataset for us\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/documents/code/ChEmbed/notebooks/../../ChEmbed/training/trainer.py:129\u001b[0m, in \u001b[0;36mTrainer.fit_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_batch_id, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_dataloader):\n\u001b[1;32m    128\u001b[0m             batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_batch_to_gpu(batch)\n\u001b[0;32m--> 129\u001b[0m             val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvalidation_step(batch)\n\u001b[1;32m    131\u001b[0m     avg_val_loss \u001b[38;5;241m=\u001b[39m val_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_val_batches\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "model_trainer = trainer.Trainer(max_epochs=16, init_random=None, clip_grads_norm=3)\n",
    "model_trainer.fit(model, chembl_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "af858e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2', 'OCc2ccccc2']\n",
      "COCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2OCc2ccccc2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_57294/1924744824.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(dataset.encode_smiles_to_one_hot(outputs[i]), device=device)\n"
     ]
    }
   ],
   "source": [
    "print(predict(\"C\", 30, model, chembl_mini, device=\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141335fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = utilities.extract_training_losses(model_trainer.metadata)\n",
    "fig, ax = plots.plot_training_validation_loss(losses['avg_train_losses'], losses['avg_val_losses'])\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ee3210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b32b7ff-53ea-47f5-a734-6b8812e93947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a5c3dd-5cdb-42dc-abbd-16379cc5eaa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
