{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d4c71d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "368d1c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling PyTorch because PyTorch >= 2.1 is required but found 2.0.0\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "import torch\n",
    "\n",
    "from ChEmbed.data import chembldb, smiles_dataset, chembed_tokenize, shakespeare_dataset\n",
    "from ChEmbed.training import trainer\n",
    "from ChEmbed.modules import simple_rnn\n",
    "import attr\n",
    "\n",
    "from ChEmbed import plots, utilities\n",
    "\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import attrs\n",
    "from torch import nn, optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df87b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_generate(prefix, num_chars, model, char_to_idx_mapping, idx_to_char_mapping, temperature = 0.0, device=None):\n",
    "    \"\"\"\n",
    "    Simple character-by-character generation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def decode_indices_to_string(encoded_indices: list, idx_to_char_mapping: dict[int, str]):\n",
    "        decoded = ''.join([idx_to_char_mapping[int(inx)] for inx in encoded_indices])\n",
    "        return decoded\n",
    "\n",
    "    def encode_string_to_indices(smiles_string: str, char_to_idx_mapping: dict[str, int]):\n",
    "        encoded = [char_to_idx_mapping[c] for c in smiles_string]\n",
    "        return encoded\n",
    "\n",
    "    model.eval()\n",
    "    generated = prefix\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_chars):\n",
    "            # Encode current text\n",
    "            encoded = torch.nn.functional.one_hot(torch.tensor(encode_string_to_indices(generated, char_to_idx_mapping)), num_classes=len(char_to_idx_mapping))\n",
    "            input_tensor = torch.tensor(encoded, device=device, dtype=torch.float32)\n",
    "            \n",
    "            # Get prediction\n",
    "            output = model(input_tensor.unsqueeze(0))  # Add batch dim\n",
    "            \n",
    "            # Get most likely next token\n",
    "            if temperature > 0:\n",
    "                # Apply temperature scaling\n",
    "                output = output / temperature\n",
    "                probabilities = torch.softmax(output, dim=-1)\n",
    "                next_token = torch.multinomial(probabilities[0, -1, :], num_samples=1).item()\n",
    "            else:\n",
    "                # Default to argmax if temperature is 0\n",
    "                next_token = output[0, -1, :].argmax().item()\n",
    "            \n",
    "            # Decode and append\n",
    "            next_char = decode_indices_to_string([next_token], idx_to_char_mapping)\n",
    "            generated += next_char\n",
    "            \n",
    "            # print(f\"Step {i+1}: Added '{next_char}' -> '{generated}'\")\n",
    "            \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f09c6a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_indices_to_string(encoded_indices: list, idx_to_char_mapping: dict[int, str]):\n",
    "    decoded = ''.join([idx_to_char_mapping[int(inx)] for inx in encoded_indices])\n",
    "    return decoded\n",
    "\n",
    "def encode_string_to_indices(smiles_string: str, char_to_idx_mapping: dict[str, int]):\n",
    "    encoded = [char_to_idx_mapping[c] for c in smiles_string]\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb1ad37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shakespeare_raw = ' '.join(open(\"../raw-data/shakespeare.txt\", 'r', encoding='utf-8').read().split('\\n'))\n",
    "shakespeare_raw = open(\"../raw-data/shakespeare.txt\", 'r', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "97fd1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = shakespeare_dataset.characterLevelShakespeare(\n",
    "    all_text = shakespeare_raw,\n",
    "    length = 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cb0bb004",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_rnn.simpleRNN(\n",
    "    # Mandatory\n",
    "    num_hiddens = 512,\n",
    "    vocab_size = len(dataset.characters),\n",
    "    # tuning\n",
    "    learning_rate = 0.001,\n",
    "    weight_decay = 1e-4,\n",
    "    num_layers = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "703c7d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 14/1064... (Epoch 1/64)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64: Train Loss: 1.9698, Val Loss: 1.6765\n",
      "Epoch 2/64: Train Loss: 1.5996, Val Loss: 1.5885\n",
      "Epoch 3/64: Train Loss: 1.5301, Val Loss: 1.5690\n",
      "Epoch 4/64: Train Loss: 1.4967, Val Loss: 1.5475\n",
      "Epoch 5/64: Train Loss: 1.4762, Val Loss: 1.5341\n",
      "Epoch 6/64: Train Loss: 1.4613, Val Loss: 1.5315\n",
      "Epoch 7/64: Train Loss: 1.4500, Val Loss: 1.5163\n",
      "Epoch 8/64: Train Loss: 1.4417, Val Loss: 1.5156\n",
      "Epoch 9/64: Train Loss: 1.4342, Val Loss: 1.5098\n",
      "Epoch 10/64: Train Loss: 1.4274, Val Loss: 1.5104\n",
      "Epoch 11/64: Train Loss: 1.4220, Val Loss: 1.5009\n",
      "Epoch 12/64: Train Loss: 1.4177, Val Loss: 1.5070\n",
      "Epoch 13/64: Train Loss: 1.4131, Val Loss: 1.4994\n",
      "Epoch 14/64: Train Loss: 1.4096, Val Loss: 1.4982\n",
      "Epoch 15/64: Train Loss: 1.4064, Val Loss: 1.5011\n",
      "Epoch 16/64: Train Loss: 1.4032, Val Loss: 1.4980\n",
      "Epoch 17/64: Train Loss: 1.4002, Val Loss: 1.4921\n",
      "Epoch 18/64: Train Loss: 1.3980, Val Loss: 1.4909\n",
      "Epoch 19/64: Train Loss: 1.3955, Val Loss: 1.4907\n",
      "Epoch 20/64: Train Loss: 1.3934, Val Loss: 1.4863\n",
      "Epoch 21/64: Train Loss: 1.3919, Val Loss: 1.4866\n",
      "Epoch 22/64: Train Loss: 1.3906, Val Loss: 1.4867\n",
      "Epoch 23/64: Train Loss: 1.3884, Val Loss: 1.4867\n",
      "Epoch 24/64: Train Loss: 1.3865, Val Loss: 1.4842\n",
      "Epoch 25/64: Train Loss: 1.3856, Val Loss: 1.4845\n",
      "Epoch 26/64: Train Loss: 1.3840, Val Loss: 1.4841\n",
      "Epoch 27/64: Train Loss: 1.3829, Val Loss: 1.4837\n",
      "Epoch 28/64: Train Loss: 1.3819, Val Loss: 1.4812\n",
      "Epoch 29/64: Train Loss: 1.3807, Val Loss: 1.4849\n",
      "Epoch 30/64: Train Loss: 1.3795, Val Loss: 1.4872\n",
      "Epoch 31/64: Train Loss: 1.3786, Val Loss: 1.4837\n",
      "Epoch 32/64: Train Loss: 1.3778, Val Loss: 1.4786\n",
      "Epoch 33/64: Train Loss: 1.3771, Val Loss: 1.4831\n",
      "Epoch 34/64: Train Loss: 1.3759, Val Loss: 1.4846\n",
      "Epoch 35/64: Train Loss: 1.3753, Val Loss: 1.4847\n",
      "Epoch 36/64: Train Loss: 1.3751, Val Loss: 1.4859\n",
      "Epoch 37/64: Train Loss: 1.3740, Val Loss: 1.4838\n",
      "Epoch 38/64: Train Loss: 1.3735, Val Loss: 1.4829\n",
      "Epoch 39/64: Train Loss: 1.3727, Val Loss: 1.4814\n",
      "Epoch 40/64: Train Loss: 1.3720, Val Loss: 1.4833\n",
      "Epoch 41/64: Train Loss: 1.3718, Val Loss: 1.4772\n",
      "Epoch 42/64: Train Loss: 1.3710, Val Loss: 1.4799\n",
      "Epoch 43/64: Train Loss: 1.3706, Val Loss: 1.4795\n",
      "Epoch 44/64: Train Loss: 1.3700, Val Loss: 1.4755\n",
      "Epoch 45/64: Train Loss: 1.3692, Val Loss: 1.4776\n",
      "Epoch 46/64: Train Loss: 1.3686, Val Loss: 1.4796\n",
      "Epoch 47/64: Train Loss: 1.3683, Val Loss: 1.4757\n",
      "Epoch 48/64: Train Loss: 1.3681, Val Loss: 1.4741\n",
      "Epoch 49/64: Train Loss: 1.3678, Val Loss: 1.4768\n",
      "Epoch 50/64: Train Loss: 1.3668, Val Loss: 1.4768\n",
      "Epoch 51/64: Train Loss: 1.3670, Val Loss: 1.4757\n",
      "Epoch 52/64: Train Loss: 1.3664, Val Loss: 1.4789\n",
      "Epoch 53/64: Train Loss: 1.3655, Val Loss: 1.4798\n",
      "Epoch 54/64: Train Loss: 1.3654, Val Loss: 1.4759\n",
      "Epoch 55/64: Train Loss: 1.3650, Val Loss: 1.4782\n",
      "Epoch 56/64: Train Loss: 1.3643, Val Loss: 1.4783\n",
      "Epoch 57/64: Train Loss: 1.3645, Val Loss: 1.4679\n",
      "Epoch 58/64: Train Loss: 1.3636, Val Loss: 1.4756\n",
      "Epoch 59/64: Train Loss: 1.3637, Val Loss: 1.4783\n",
      "Epoch 60/64: Train Loss: 1.3638, Val Loss: 1.4739\n",
      "Epoch 61/64: Train Loss: 1.3629, Val Loss: 1.4758\n",
      "Epoch 62/64: Train Loss: 1.3628, Val Loss: 1.4734\n",
      "Epoch 63/64: Train Loss: 1.3624, Val Loss: 1.4771\n",
      "Epoch 64/64: Train Loss: 1.3624, Val Loss: 1.4810\n"
     ]
    }
   ],
   "source": [
    "model_trainer = trainer.Trainer(max_epochs=64, init_random=None, clip_grads_norm=3.0)\n",
    "model_trainer.fit(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d0092f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_133582/2132123058.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_tensor = torch.tensor(encoded, device=device, dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input and parameter tensors are not at the same device, found input tensor at cuda:0 and parameter tensor at cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msimple_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClaudius:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchar_to_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midx_to_char\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[34], line 24\u001b[0m, in \u001b[0;36msimple_generate\u001b[0;34m(prefix, num_chars, model, char_to_idx_mapping, idx_to_char_mapping, temperature, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(encoded, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Get prediction\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Add batch dim\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Get most likely next token\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m temperature \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Apply temperature scaling\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/documents/code/ChEmbed/notebooks/../../ChEmbed/modules/simple_rnn.py:24\u001b[0m, in \u001b[0;36msimpleRNN.forward\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 24\u001b[0m     output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(output)\n\u001b[1;32m     26\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(output)\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/rnn.py:509\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN_TANH\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 509\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn_tanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    513\u001b[0m         result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mrnn_relu(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    514\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m    515\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input and parameter tensors are not at the same device, found input tensor at cuda:0 and parameter tensor at cpu"
     ]
    }
   ],
   "source": [
    "print(simple_generate(\"Claudius:\", 100, model, dataset.char_to_idx, dataset.idx_to_char, temperature=1.5, device='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "928d2c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = utilities.extract_training_losses(model_trainer.metadata)\n",
    "fig, ax = plots.plot_training_validation_loss(losses['avg_train_losses'], losses['avg_val_losses'])\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ac9423fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([128, 31, 84])\n",
      "Target shape: torch.Size([128, 31])\n",
      "Output shape: torch.Size([128, 31, 84])\n",
      "Loss value: 1.293547511100769\n",
      "Preds shape: torch.Size([3968, 84])\n",
      "Targets shape: torch.Size([3968])\n",
      "Sample predictions (first 10 probs): tensor([[7.6064e-04, 5.3947e-05, 3.5710e-03, 1.3728e-05, 2.5448e-05, 1.1104e-02,\n",
      "         3.7910e-01, 1.6492e-02, 6.5295e-02, 9.9822e-03, 2.3556e-05, 5.6298e-02,\n",
      "         2.2327e-02, 7.0285e-03, 3.0562e-05, 1.0323e-05, 1.8616e-03, 2.3812e-03,\n",
      "         6.8865e-03, 2.5878e-02, 4.2529e-05, 1.4947e-05, 7.5945e-05, 1.7665e-03,\n",
      "         1.0521e-02, 7.7659e-03, 2.5454e-05, 3.4138e-03, 1.4623e-05, 2.5096e-02,\n",
      "         8.4331e-03, 9.9489e-03, 4.2755e-03, 2.4988e-05, 9.2273e-06, 6.4999e-04,\n",
      "         4.0768e-05, 2.4791e-05, 4.7395e-04, 9.9262e-04, 2.4562e-05, 1.0521e-03,\n",
      "         3.1456e-02, 2.7616e-03, 1.3102e-03, 1.0353e-02, 9.7878e-03, 4.1436e-03,\n",
      "         2.9762e-05, 4.2440e-03, 1.5002e-05, 7.2652e-04, 1.1266e-03, 2.6571e-05,\n",
      "         1.0112e-04, 6.7999e-03, 3.2905e-05, 4.7149e-05, 7.3898e-04, 3.9539e-02,\n",
      "         3.4815e-02, 1.5590e-02, 2.8801e-05, 1.7899e-02, 6.3096e-03, 1.2739e-05,\n",
      "         2.2695e-05, 9.0552e-03, 4.2239e-02, 5.9702e-03, 7.3083e-05, 3.8392e-03,\n",
      "         3.9263e-05, 4.1590e-03, 8.8916e-03, 1.2108e-02, 1.7800e-02, 1.3194e-02,\n",
      "         1.3554e-03, 1.7392e-03, 1.8533e-03, 1.5266e-05, 5.5964e-03, 3.4492e-04],\n",
      "        [1.6632e-03, 6.1704e-05, 4.1122e-03, 2.4798e-05, 1.1461e-05, 1.1388e-02,\n",
      "         7.8972e-01, 4.9267e-04, 4.4654e-03, 5.9771e-03, 6.1987e-06, 5.0174e-03,\n",
      "         5.4089e-04, 1.0217e-02, 2.2451e-05, 3.8324e-06, 1.0394e-03, 7.2026e-03,\n",
      "         7.6116e-03, 1.1186e-03, 1.3869e-04, 2.4440e-05, 4.8140e-05, 2.6855e-03,\n",
      "         7.6639e-03, 4.2258e-04, 3.4123e-05, 1.0728e-04, 1.4186e-05, 7.9454e-04,\n",
      "         4.1428e-04, 8.3469e-03, 3.3845e-03, 9.4256e-06, 2.5241e-05, 3.4310e-04,\n",
      "         1.6561e-05, 2.0430e-05, 8.9068e-04, 8.5502e-05, 1.0065e-05, 2.6843e-04,\n",
      "         2.9646e-04, 1.1744e-03, 2.8276e-03, 1.0063e-02, 4.9148e-03, 1.8756e-02,\n",
      "         1.3984e-05, 8.8159e-05, 1.8357e-05, 4.2683e-04, 8.9024e-04, 3.6135e-05,\n",
      "         1.8348e-04, 4.0984e-03, 5.7059e-05, 4.0685e-05, 1.2140e-04, 1.4540e-03,\n",
      "         1.6271e-03, 2.8240e-02, 1.5172e-05, 1.2971e-03, 3.5375e-03, 3.6122e-06,\n",
      "         2.0162e-05, 2.2818e-04, 2.4486e-03, 7.6639e-04, 2.6543e-05, 2.2835e-03,\n",
      "         5.2394e-05, 4.9326e-03, 2.6216e-02, 7.7778e-04, 6.0597e-04, 1.0540e-03,\n",
      "         1.5641e-03, 1.6329e-03, 2.0812e-05, 6.7171e-07, 5.9005e-04, 1.5448e-04],\n",
      "        [1.7458e-03, 1.9000e-05, 6.3947e-03, 2.6239e-05, 1.1543e-05, 1.7070e-02,\n",
      "         7.4995e-01, 5.9233e-04, 1.7909e-03, 8.1392e-03, 6.1698e-06, 2.2847e-03,\n",
      "         5.5501e-04, 1.5460e-02, 2.8153e-05, 3.7125e-06, 9.8739e-04, 1.0749e-02,\n",
      "         1.4213e-02, 8.1668e-04, 3.2411e-05, 1.1560e-05, 3.5591e-05, 5.3662e-03,\n",
      "         8.9010e-03, 5.2753e-04, 2.1956e-05, 6.8982e-05, 1.7496e-05, 8.2864e-04,\n",
      "         3.6082e-04, 4.0331e-03, 2.2254e-03, 6.9611e-06, 2.4805e-05, 2.2210e-04,\n",
      "         1.4660e-05, 9.5414e-06, 2.1667e-04, 9.3392e-05, 7.1411e-06, 3.1584e-04,\n",
      "         9.5472e-04, 4.6518e-04, 2.8941e-03, 2.2990e-02, 8.6841e-03, 1.4533e-03,\n",
      "         6.7691e-06, 2.2504e-04, 6.4379e-06, 1.2841e-03, 3.8414e-04, 1.2718e-05,\n",
      "         6.3184e-05, 7.5911e-03, 1.9393e-05, 4.2090e-05, 7.2016e-05, 1.9953e-03,\n",
      "         1.1599e-03, 3.8956e-02, 1.5933e-05, 4.9291e-04, 6.7844e-03, 5.0537e-06,\n",
      "         6.5712e-06, 4.0212e-04, 4.1195e-03, 4.8801e-04, 2.0632e-05, 3.6992e-03,\n",
      "         3.0301e-05, 7.4391e-03, 2.7807e-02, 6.9635e-04, 1.0456e-03, 8.0383e-04,\n",
      "         1.6504e-03, 4.7896e-04, 4.1694e-05, 2.0960e-06, 4.3772e-04, 8.8431e-05],\n",
      "        [2.5172e-03, 4.9581e-05, 5.8914e-03, 3.0469e-05, 2.6270e-05, 2.3147e-02,\n",
      "         6.8922e-01, 7.8313e-04, 3.4628e-03, 9.4657e-03, 1.1549e-05, 3.9414e-03,\n",
      "         9.8419e-04, 2.5013e-02, 8.7690e-05, 8.2449e-06, 1.2011e-03, 1.0855e-02,\n",
      "         2.4054e-02, 1.1857e-03, 1.2544e-05, 1.0675e-05, 1.0714e-04, 7.3144e-03,\n",
      "         8.2196e-03, 4.5249e-04, 5.2398e-05, 1.3648e-04, 2.5916e-05, 1.8698e-03,\n",
      "         5.3555e-04, 7.6750e-03, 5.0734e-03, 1.2995e-05, 6.1731e-05, 1.5999e-04,\n",
      "         3.6000e-05, 9.0000e-06, 4.5557e-04, 1.2241e-04, 1.3232e-05, 7.8520e-04,\n",
      "         9.2885e-04, 7.0117e-04, 7.4744e-03, 2.6423e-02, 7.3292e-03, 1.1091e-03,\n",
      "         1.5262e-05, 2.0538e-04, 1.0874e-05, 1.3510e-03, 4.9151e-04, 8.3373e-06,\n",
      "         4.5034e-05, 7.0497e-03, 1.3496e-05, 1.0104e-04, 1.1069e-04, 3.8608e-03,\n",
      "         1.5910e-03, 3.0798e-02, 3.9045e-05, 6.8488e-04, 3.5366e-03, 1.9997e-05,\n",
      "         9.0672e-06, 8.8798e-04, 2.9663e-03, 8.7558e-04, 3.1474e-05, 4.9737e-03,\n",
      "         5.0004e-05, 1.0240e-02, 4.2092e-02, 9.6472e-04, 9.7936e-04, 1.6805e-03,\n",
      "         4.0310e-03, 8.1812e-04, 3.4793e-05, 3.6743e-06, 2.4054e-04, 1.4096e-04],\n",
      "        [9.4268e-04, 2.3055e-05, 4.5365e-04, 1.8334e-05, 9.5029e-06, 2.1754e-03,\n",
      "         9.4873e-01, 5.2284e-05, 4.2070e-04, 4.9792e-04, 4.4763e-06, 4.8181e-04,\n",
      "         8.0254e-05, 1.8562e-02, 4.8869e-05, 2.1442e-06, 3.2163e-04, 1.6147e-03,\n",
      "         1.6937e-03, 5.7508e-05, 1.8014e-05, 1.1316e-05, 5.7322e-05, 3.6362e-04,\n",
      "         6.2671e-04, 7.5167e-05, 2.1934e-05, 2.1562e-05, 7.1743e-06, 2.0519e-04,\n",
      "         1.4346e-04, 7.2803e-04, 9.6106e-04, 3.1155e-06, 3.3613e-05, 1.6763e-05,\n",
      "         1.2650e-05, 7.1290e-06, 2.6686e-04, 1.1768e-05, 3.1512e-06, 8.5619e-05,\n",
      "         9.9715e-05, 6.6331e-05, 6.2568e-03, 1.8910e-03, 4.0008e-04, 1.2925e-03,\n",
      "         4.8889e-06, 1.2268e-05, 3.2903e-06, 5.2608e-05, 7.3690e-05, 1.1109e-05,\n",
      "         6.1951e-05, 3.5874e-04, 1.7880e-05, 5.3146e-05, 6.8508e-06, 3.1558e-04,\n",
      "         1.5628e-04, 1.5676e-03, 1.9176e-05, 4.9313e-05, 5.3593e-04, 7.1544e-06,\n",
      "         1.1358e-05, 9.4583e-05, 5.1203e-04, 1.2227e-04, 5.5208e-06, 5.3366e-04,\n",
      "         1.3658e-05, 1.0985e-03, 3.1501e-03, 1.5167e-04, 8.9095e-05, 1.2553e-04,\n",
      "         6.2488e-04, 1.5469e-04, 2.3806e-06, 8.3430e-07, 1.8128e-05, 1.0440e-04],\n",
      "        [3.5303e-04, 1.5792e-05, 2.8004e-04, 1.2515e-05, 7.3428e-06, 9.0626e-04,\n",
      "         9.6496e-01, 1.1759e-05, 1.0854e-04, 2.1108e-04, 3.2823e-06, 1.3465e-04,\n",
      "         3.0278e-05, 1.8151e-02, 4.6322e-05, 1.1224e-06, 1.9980e-04, 1.1402e-03,\n",
      "         1.0419e-03, 2.3751e-05, 1.3926e-05, 8.2416e-06, 5.4280e-05, 1.5178e-04,\n",
      "         2.8696e-04, 1.9064e-05, 2.0381e-05, 9.1494e-06, 3.0759e-06, 6.8794e-05,\n",
      "         5.6212e-05, 2.6544e-04, 3.7397e-04, 1.8061e-06, 3.3543e-05, 6.4168e-06,\n",
      "         9.0453e-06, 4.8325e-06, 2.4579e-04, 4.0195e-06, 1.8345e-06, 5.0165e-05,\n",
      "         3.5651e-05, 3.3474e-05, 4.6476e-03, 8.0589e-04, 1.8959e-04, 5.3893e-04,\n",
      "         3.0390e-06, 5.0474e-06, 1.7072e-06, 4.2108e-05, 4.8194e-05, 7.9125e-06,\n",
      "         4.7514e-05, 2.0330e-04, 1.5864e-05, 5.2197e-05, 1.5277e-06, 1.3799e-04,\n",
      "         6.4167e-05, 5.5123e-04, 1.6605e-05, 1.0398e-05, 2.6178e-04, 5.8224e-06,\n",
      "         1.5624e-05, 3.4007e-05, 1.3842e-04, 4.5163e-05, 2.8649e-06, 2.6153e-04,\n",
      "         7.8382e-06, 6.5064e-04, 1.0096e-03, 7.1058e-05, 3.9040e-05, 5.6632e-05,\n",
      "         3.9141e-04, 1.2391e-04, 5.4928e-07, 3.7939e-07, 4.6338e-06, 8.7851e-05],\n",
      "        [2.9229e-04, 2.5618e-05, 2.9131e-04, 1.6184e-05, 1.1365e-05, 1.1404e-03,\n",
      "         9.5313e-01, 9.0517e-06, 1.0315e-04, 2.5471e-04, 4.9761e-06, 1.3578e-04,\n",
      "         3.5587e-05, 2.4686e-02, 7.5827e-05, 1.3836e-06, 2.4133e-04, 1.2918e-03,\n",
      "         1.4478e-03, 1.6127e-05, 1.8249e-05, 9.9418e-06, 8.7021e-05, 1.6734e-04,\n",
      "         3.6107e-04, 1.7554e-05, 3.2575e-05, 1.0543e-05, 4.4703e-06, 5.7075e-05,\n",
      "         6.1616e-05, 2.6430e-04, 4.3705e-04, 2.3801e-06, 6.2180e-05, 8.0599e-06,\n",
      "         1.4386e-05, 5.9214e-06, 4.1200e-04, 3.1661e-06, 2.4438e-06, 5.6888e-05,\n",
      "         4.0861e-05, 3.9888e-05, 7.6418e-03, 8.4018e-04, 2.7795e-04, 5.9258e-04,\n",
      "         3.8275e-06, 5.2196e-06, 2.3822e-06, 6.6478e-05, 5.9567e-05, 9.5051e-06,\n",
      "         5.7376e-05, 2.0696e-04, 1.9371e-05, 9.5667e-05, 1.2068e-06, 1.2959e-04,\n",
      "         5.9323e-05, 5.6072e-04, 2.6247e-05, 7.4949e-06, 3.2683e-04, 9.2963e-06,\n",
      "         2.5583e-05, 2.8161e-05, 1.2776e-04, 4.4873e-05, 3.5048e-06, 3.4639e-04,\n",
      "         1.0482e-05, 8.8957e-04, 1.1339e-03, 7.2310e-05, 3.5250e-05, 5.5277e-05,\n",
      "         5.2238e-04, 1.7102e-04, 5.6869e-07, 4.5414e-07, 4.2805e-06, 1.3977e-04],\n",
      "        [2.4567e-04, 2.6908e-05, 3.1430e-04, 1.5567e-05, 1.2505e-05, 1.2061e-03,\n",
      "         9.5306e-01, 9.4725e-06, 1.2511e-04, 2.8795e-04, 5.6267e-06, 1.4920e-04,\n",
      "         3.7049e-05, 2.3207e-02, 8.5306e-05, 1.4994e-06, 2.1431e-04, 1.2739e-03,\n",
      "         1.4110e-03, 2.0121e-05, 1.8733e-05, 9.5648e-06, 9.5570e-05, 1.7717e-04,\n",
      "         3.9627e-04, 1.9388e-05, 3.6579e-05, 9.8512e-06, 4.2799e-06, 6.3445e-05,\n",
      "         6.3607e-05, 2.9444e-04, 4.6920e-04, 2.5030e-06, 6.9525e-05, 8.9527e-06,\n",
      "         1.5922e-05, 5.7898e-06, 4.5030e-04, 3.3290e-06, 2.5752e-06, 6.2972e-05,\n",
      "         4.2893e-05, 4.1537e-05, 8.4955e-03, 9.8821e-04, 2.7878e-04, 5.2907e-04,\n",
      "         3.8326e-06, 5.3137e-06, 2.3313e-06, 7.0183e-05, 5.6681e-05, 9.4993e-06,\n",
      "         5.8104e-05, 2.2113e-04, 1.9345e-05, 1.0683e-04, 1.1661e-06, 1.3123e-04,\n",
      "         6.7734e-05, 6.0648e-04, 2.9079e-05, 8.4135e-06, 3.2630e-04, 1.0490e-05,\n",
      "         2.6117e-05, 3.4117e-05, 1.2626e-04, 4.6004e-05, 3.6106e-06, 3.2780e-04,\n",
      "         1.0475e-05, 8.9120e-04, 1.3889e-03, 7.6487e-05, 3.6746e-05, 5.7946e-05,\n",
      "         5.5207e-04, 1.6456e-04, 5.6324e-07, 4.7171e-07, 4.7583e-06, 1.5439e-04],\n",
      "        [2.5681e-04, 3.1816e-05, 3.3337e-04, 1.5340e-05, 1.4366e-05, 1.2410e-03,\n",
      "         9.4651e-01, 1.0465e-05, 1.3419e-04, 3.3560e-04, 6.4565e-06, 1.9283e-04,\n",
      "         3.9134e-05, 2.7471e-02, 9.8888e-05, 1.7993e-06, 2.1385e-04, 1.3872e-03,\n",
      "         1.3949e-03, 2.3123e-05, 2.0257e-05, 1.0101e-05, 1.1089e-04, 1.9492e-04,\n",
      "         4.2497e-04, 1.9213e-05, 4.0051e-05, 1.0314e-05, 4.3729e-06, 7.4362e-05,\n",
      "         7.4378e-05, 3.0469e-04, 5.0192e-04, 2.7973e-06, 7.5023e-05, 1.1223e-05,\n",
      "         1.8556e-05, 5.8348e-06, 5.3664e-04, 3.4650e-06, 2.8646e-06, 7.1041e-05,\n",
      "         5.2703e-05, 4.7438e-05, 9.7682e-03, 1.0224e-03, 3.3665e-04, 5.7869e-04,\n",
      "         4.4356e-06, 5.7128e-06, 2.5837e-06, 7.5324e-05, 5.8946e-05, 9.9519e-06,\n",
      "         5.9597e-05, 2.3498e-04, 1.9995e-05, 1.2563e-04, 1.2465e-06, 1.4045e-04,\n",
      "         7.2493e-05, 7.0344e-04, 3.3331e-05, 9.1338e-06, 3.5179e-04, 1.1577e-05,\n",
      "         2.9553e-05, 4.0995e-05, 1.2638e-04, 4.9322e-05, 3.9900e-06, 3.6756e-04,\n",
      "         1.1936e-05, 8.6109e-04, 1.3882e-03, 9.2984e-05, 3.5762e-05, 5.9771e-05,\n",
      "         6.0668e-04, 1.9176e-04, 6.3895e-07, 5.1942e-07, 5.3335e-06, 1.6730e-04],\n",
      "        [2.5872e-04, 3.4875e-05, 3.3840e-04, 1.5554e-05, 1.5244e-05, 1.2024e-03,\n",
      "         9.4359e-01, 1.0025e-05, 1.3324e-04, 3.6833e-04, 7.0709e-06, 1.8982e-04,\n",
      "         3.8375e-05, 2.9448e-02, 1.0748e-04, 1.8826e-06, 1.9965e-04, 1.4135e-03,\n",
      "         1.3613e-03, 2.2948e-05, 2.1958e-05, 1.0541e-05, 1.1963e-04, 2.1118e-04,\n",
      "         4.2755e-04, 1.7699e-05, 4.0627e-05, 1.0621e-05, 4.3982e-06, 7.3147e-05,\n",
      "         7.9111e-05, 2.9807e-04, 4.7775e-04, 2.9107e-06, 7.9187e-05, 1.1839e-05,\n",
      "         2.0137e-05, 5.7029e-06, 6.1657e-04, 3.3326e-06, 2.9723e-06, 7.4476e-05,\n",
      "         5.2377e-05, 4.8508e-05, 1.0489e-02, 9.8475e-04, 3.4557e-04, 6.1758e-04,\n",
      "         4.7480e-06, 5.9367e-06, 2.7014e-06, 7.6841e-05, 5.8153e-05, 1.0380e-05,\n",
      "         6.1075e-05, 2.3613e-04, 2.0708e-05, 1.3698e-04, 1.1976e-06, 1.3500e-04,\n",
      "         6.9337e-05, 6.8835e-04, 3.5861e-05, 8.8907e-06, 3.7650e-04, 1.1714e-05,\n",
      "         3.2296e-05, 4.3929e-05, 1.1923e-04, 5.1604e-05, 4.0666e-06, 3.6762e-04,\n",
      "         1.2287e-05, 8.7976e-04, 1.4312e-03, 9.6775e-05, 3.4745e-05, 5.4728e-05,\n",
      "         6.5058e-04, 1.9587e-04, 6.2263e-07, 5.1569e-07, 5.4586e-06, 1.7256e-04]],\n",
      "       device='cuda:0')\n",
      "Sample target indices (first 10): tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0')\n",
      "Are predictions roughly uniform? (should be ~1/vocab_size)\n",
      "Mean probabilities across vocab: tensor([1.0908e-04, 3.7370e-05, 3.0386e-03, 7.7851e-05, 1.8405e-05, 5.9040e-03,\n",
      "        2.3018e-01, 2.6669e-02, 5.1648e-02, 7.9670e-03], device='cuda:0')\n",
      "Expected uniform prob: 0.011904761904761904\n"
     ]
    }
   ],
   "source": [
    "# Let's test a simple batch to understand what's happening\n",
    "test_batch = next(iter(dataset.train_dataloader()))\n",
    "print(\"Input shape:\", test_batch[0].shape)\n",
    "print(\"Target shape:\", test_batch[1].shape)\n",
    "\n",
    "# Move to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "test_input = test_batch[0].to(device)\n",
    "test_target = test_batch[1].to(device)\n",
    "\n",
    "# Test model forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(test_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    \n",
    "    # Check if gradients are flowing properly by testing loss\n",
    "    loss = model.training_step([test_input, test_target])\n",
    "    print(\"Loss value:\", loss.item())\n",
    "    \n",
    "    # Let's see what the actual predictions look like\n",
    "    preds = output.reshape(-1, len(dataset.characters))\n",
    "    targets = test_target.reshape(-1)\n",
    "    print(\"Preds shape:\", preds.shape)\n",
    "    print(\"Targets shape:\", targets.shape)\n",
    "    print(\"Sample predictions (first 10 probs):\", torch.softmax(preds[:10], dim=1))\n",
    "    print(\"Sample target indices (first 10):\", targets[:10])\n",
    "    \n",
    "    # Check if predictions are just random/uniform\n",
    "    print(\"Are predictions roughly uniform? (should be ~1/vocab_size)\")\n",
    "    mean_probs = torch.softmax(preds, dim=1).mean(dim=0)\n",
    "    print(\"Mean probabilities across vocab:\", mean_probs[:10])\n",
    "    print(\"Expected uniform prob:\", 1.0/len(dataset.characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7d1d084f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 84\n",
      "Expected uniform probability: 0.011904761904761904\n",
      "First prediction probabilities (should be diverse if learning):\n",
      "Max prob: 0.3791000545024872\n",
      "Min prob: 9.227306691172998e-06\n",
      "Std dev: 0.042424771934747696\n",
      "Are all predictions identical? False\n"
     ]
    }
   ],
   "source": [
    "# Simple test - are predictions changing or stuck?\n",
    "test_batch = next(iter(dataset.train_dataloader()))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "test_input = test_batch[0].to(device)\n",
    "test_target = test_batch[1].to(device)\n",
    "\n",
    "print(\"Vocab size:\", len(dataset.characters))\n",
    "print(\"Expected uniform probability:\", 1.0/len(dataset.characters))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(test_input)\n",
    "    preds = torch.softmax(output.reshape(-1, len(dataset.characters)), dim=1)\n",
    "    \n",
    "    # Check first prediction probabilities\n",
    "    first_pred = preds[0]\n",
    "    print(\"First prediction probabilities (should be diverse if learning):\")\n",
    "    print(\"Max prob:\", first_pred.max().item())\n",
    "    print(\"Min prob:\", first_pred.min().item())\n",
    "    print(\"Std dev:\", first_pred.std().item())\n",
    "    \n",
    "    # Check if all predictions are the same (stuck)\n",
    "    all_same = torch.allclose(preds[0], preds[1], atol=1e-6)\n",
    "    print(\"Are all predictions identical?\", all_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bb0dce2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 84\n",
      "Theoretical minimum loss (random guessing): 4.4308\n",
      "Current loss (~2.5) vs theoretical min: 0.56x higher\n",
      "\n",
      "=== Gradient Analysis ===\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     33\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 34\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mdiagnostic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_target\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     37\u001b[0m     total_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/documents/code/ChEmbed/notebooks/../../ChEmbed/modules/simple_rnn.py:54\u001b[0m, in \u001b[0;36msimpleRNN.training_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# print(batch[0].shape)  # Debugging: Check input shape]\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# print(batch[1].shape)  # Debugging: Check target shape\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# Flatten for cross-entropy loss\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# preds: (batch_size, seq_len, vocab_size) -> (batch_size * seq_len, vocab_size)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# labels: (batch_size, seq_len) -> (batch_size * seq_len)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/documents/code/ChEmbed/notebooks/../../ChEmbed/modules/simple_rnn.py:26\u001b[0m, in \u001b[0;36msimpleRNN.forward\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m     24\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(inputs, state)\n\u001b[1;32m     25\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(output)\n\u001b[0;32m---> 26\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility"
     ]
    }
   ],
   "source": [
    "# Let's diagnose the specific issues with training\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# Check theoretical minimum loss\n",
    "vocab_size = len(dataset.characters)\n",
    "theoretical_min_loss = -math.log(1.0/vocab_size)\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Theoretical minimum loss (random guessing): {theoretical_min_loss:.4f}\")\n",
    "print(f\"Current loss (~2.5) vs theoretical min: {2.5/theoretical_min_loss:.2f}x higher\")\n",
    "\n",
    "# Test gradients\n",
    "test_batch = next(iter(dataset.train_dataloader()))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "test_input = test_batch[0].to(device)\n",
    "test_target = test_batch[1].to(device)\n",
    "\n",
    "# Create a fresh model for testing\n",
    "diagnostic_model = simple_rnn.simpleRNN(\n",
    "    num_hiddens=64,\n",
    "    vocab_size=len(dataset.characters),\n",
    "    learning_rate=0.01,  # Note: using 0.01, not 0.1\n",
    "    weight_decay=0.01\n",
    ")\n",
    "diagnostic_model.to(device)\n",
    "\n",
    "# Check gradient magnitudes\n",
    "diagnostic_model.train()\n",
    "optimizer = diagnostic_model.configure_optimizers()\n",
    "\n",
    "print(\"\\n=== Gradient Analysis ===\")\n",
    "for step in range(3):\n",
    "    optimizer.zero_grad()\n",
    "    loss = diagnostic_model.training_step([test_input, test_target])\n",
    "    loss.backward()\n",
    "    \n",
    "    total_norm = 0\n",
    "    param_count = 0\n",
    "    for name, param in diagnostic_model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            param_norm = param.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "            param_count += 1\n",
    "            print(f\"Step {step+1} - {name}: grad_norm = {param_norm:.6f}\")\n",
    "    \n",
    "    total_norm = total_norm ** (1. / 2)\n",
    "    print(f\"Step {step+1} - Total gradient norm: {total_norm:.6f}\")\n",
    "    print(f\"Step {step+1} - Loss: {loss.item():.6f}\")\n",
    "    \n",
    "    optimizer.step()\n",
    "    print()\n",
    "\n",
    "# Check if predictions are collapsing to uniform distribution\n",
    "print(\"=== Prediction Analysis ===\")\n",
    "diagnostic_model.eval()\n",
    "with torch.no_grad():\n",
    "    output = diagnostic_model(test_input[:1])  # Just one sample\n",
    "    probs = torch.softmax(output.reshape(-1, vocab_size), dim=1)\n",
    "    \n",
    "    # Check entropy - should be high if uniform, low if confident\n",
    "    entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=1).mean()\n",
    "    max_entropy = math.log(vocab_size)\n",
    "    \n",
    "    print(f\"Average prediction entropy: {entropy:.4f}\")\n",
    "    print(f\"Maximum possible entropy: {max_entropy:.4f}\")\n",
    "    print(f\"Entropy ratio (1.0 = completely uniform): {entropy/max_entropy:.4f}\")\n",
    "    \n",
    "    # Check if model is just outputting the same thing\n",
    "    std_across_vocab = probs.std(dim=1).mean()\n",
    "    print(f\"Std dev across vocabulary: {std_across_vocab:.6f}\")\n",
    "    print(f\"Expected std for uniform dist: {(1/vocab_size * (1-1/vocab_size))**0.5:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91aa933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
